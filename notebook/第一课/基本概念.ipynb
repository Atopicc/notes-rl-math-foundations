{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强化学习基本概念\n",
    "\n",
    "本章主要讲解强化学习中的核心概念，包括状态（state）、动作（action）、奖励（reward）、回报（return）、回合（episode）、策略（policy）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid World 示例\n",
    "\n",
    "在本课程中，我们广泛使用 **Grid World** 作为示例场景，因为它简单直观，易于理解。\n",
    "\n",
    "- 机器人在不同的网格（grid）中移动。\n",
    "- 网格有不同的状态：可访问（Accessible）、禁止（forbidden）、目标（target）以及地图边界（boundary）。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Grid World 示意图](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 目标与路径优化\n",
    "\n",
    "在示例中，机器人需要找到一条“好”的路径到达目标（target）。如何定义“好”的路径？\n",
    "\n",
    "- 路径短\n",
    "- 避免障碍\n",
    "- 远离边界"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 核心概念详解\n",
    "\n",
    "下面我们将逐步介绍强化学习中的基本概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State（状态）\n",
    "\n",
    "**State** 表示智能体（agent）在环境中的当前状态。\n",
    "\n",
    "- 在 Grid World 中，状态就是机器人的位置，例如 $s_1, s_2, \\dots, s_9$。\n",
    "- **State Space（状态空间）**：所有可能状态的集合，记为 $S = \\{s_i\\}_{i=1}^9$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Action（动作）\n",
    "\n",
    "**Action** 是智能体在每个状态可以采取的行动。\n",
    "\n",
    "- 在 Grid World 中，每个位置可以向四个方向移动或保持不动，编码为 $a_1, a_2, \\dots, a_5$。\n",
    "- **Action Space（动作空间）**：在状态 $s_i$ 下所有可行动作的集合，记为 $A(s_i) = \\{a_i\\}_{i=1}^5$。\n",
    "\n",
    "**问题**：不同的状态能否有不同的动作集合？答案是肯定的，例如边界状态可能限制某些动作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Transition（状态转移）\n",
    "\n",
    "采取行动后，智能体会从一个状态转移到另一个状态，这个过程称为 **State Transition**。\n",
    "\n",
    "- 状态转移定义了智能体与环境的交互。\n",
    "- 例如：$s_1 \\xrightarrow{a_1} s_2$。\n",
    "\n",
    "状态转移可以是确定性的（deterministic）或随机性的（stochastic）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forbidden Area（禁止区域）\n",
    "\n",
    "禁止区域有两种常见情况：\n",
    "\n",
    "1. **可进入但受惩罚**：智能体可以进入，但会收到负奖励。\n",
    "2. **无法进入**：智能体无法通过动作进入该区域。\n",
    "\n",
    "本课程主要关注第一种情况，因为它更具挑战性，状态空间更大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 状态转移的表格表示\n",
    "\n",
    "以下表格描述了确定性的状态转移（deterministic case）：\n",
    "\n",
    "| State | $a_1$ (up) | $a_2$ (right) | $a_3$ (down) | $a_4$ (left) | $a_5$ (unchanged) |\n",
    "|:-----:|:----------:|:-------------:|:------------:|:------------:|:-----------------:|\n",
    "| $s_1$ |    $s_1$   |     $s_2$     |     $s_4$    |     $s_1$    |       $s_1$       |\n",
    "| $s_2$ |    $s_2$   |     $s_3$     |     $s_5$    |     $s_1$    |       $s_2$       |\n",
    "| $s_3$ |    $s_3$   |     $s_3$     |     $s_6$    |     $s_2$    |       $s_3$       |\n",
    "| $s_4$ |    $s_1$   |     $s_5$     |     $s_7$    |     $s_4$    |       $s_4$       |\n",
    "| $s_5$ |    $s_2$   |     $s_6$     |     $s_8$    |     $s_4$    |       $s_5$       |\n",
    "| $s_6$ |    $s_3$   |     $s_6$     |     $s_9$    |     $s_5$    |       $s_6$       |\n",
    "| $s_7$ |    $s_4$   |     $s_8$     |     $s_7$    |     $s_7$    |       $s_7$       |\n",
    "| $s_8$ |    $s_5$   |     $s_9$     |     $s_8$    |     $s_7$    |       $s_8$       |\n",
    "| $s_9$ |    $s_6$   |     $s_9$     |     $s_9$    |     $s_8$    |       $s_9$       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 状态转移概率\n",
    "\n",
    "更一般的方法是使用 **State Transition Probability** 来描述随机性状态转移（stochastic case）。\n",
    "\n",
    "- 数学描述：\n",
    "  $$\n",
    "  P(s_2 \\mid s_1, a_2) = 1\n",
    "  $$\n",
    "  $$\n",
    "  P(s_i \\mid s_1, a_2) = 0, \\quad \\forall i \\neq 2\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy（策略）\n",
    "\n",
    "**Policy** 告诉智能体在某个状态下应该采取什么动作。通常用箭头或概率表示，从而形成一条路径（path）。\n",
    "\n",
    "例如，在 Grid World 中，策略可以用箭头表示移动方向："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![策略示意图](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略的数学表示\n",
    "\n",
    "对于状态 $s_1$，策略 $\\pi$ 可以表示为：\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\pi(a_1 \\mid s_1) &= 0 \\\\\n",
    "\\pi(a_2 \\mid s_1) &= 1 \\\\\n",
    "\\pi(a_3 \\mid s_1) &= 0 \\\\\n",
    "\\pi(a_4 \\mid s_1) &= 0 \\\\\n",
    "\\pi(a_5 \\mid s_1) &= 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "所有策略概率之和为 1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 策略的表格表示\n",
    "\n",
    "以下表格展示了一个随机策略的示例：\n",
    "\n",
    "| State | $a_1$ (up) | $a_2$ (right) | $a_3$ (down) | $a_4$ (left) | $a_5$ (unchanged) |\n",
    "|:-----:|:----------:|:-------------:|:------------:|:------------:|:-----------------:|\n",
    "| $s_1$ |     0      |      0.5      |      0.5     |      0       |         0         |\n",
    "| $s_2$ |     0      |       0       |      1       |      0       |         0         |\n",
    "| $s_3$ |     0      |       0       |      0       |      1       |         0         |\n",
    "| $s_4$ |     0      |       1       |      0       |      0       |         0         |\n",
    "| $s_5$ |     0      |       0       |      1       |      0       |         0         |\n",
    "| $s_6$ |     0      |       0       |      1       |      0       |         0         |\n",
    "| $s_7$ |     0      |       1       |      0       |      0       |         0         |\n",
    "| $s_8$ |     0      |       1       |      0       |      0       |         0         |\n",
    "| $s_9$ |     0      |       0       |      0       |      0       |         1         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward（奖励）\n",
    "\n",
    "**Reward** 是环境在智能体采取动作后给予的反馈信号，通常是一个标量数值。\n",
    "\n",
    "- 奖励定义了任务的目标：**正奖励**鼓励某种行为，**负奖励**（惩罚）抑制某种行为。\n",
    "- 它可以只依赖于当前状态 $R(s)$，也可以依赖于状态-动作对 $R(s, a)$。\n",
    "\n",
    "#### Grid World 中的奖励设计\n",
    "在本示例中，我们希望机器人尽快到达目标并避开禁区。一种常见的奖励设计如下：\n",
    "\n",
    "| 场景 | 奖励值 ($r$) | 目的 |\n",
    "|:---|:---:|:---|\n",
    "| 到达目标 (Target) | $+1$ | 鼓励到达终点 |\n",
    "| 进入禁区 (Forbidden) | $-10$ | 强烈惩罚进入危险区域 |\n",
    "| 撞墙 (Boundary) | $-1$ | 惩罚无效移动 |\n",
    "| 其它每一步 (Step) | $-0.02$ | 鼓励寻找最短路径（时间成本） |\n",
    "\n",
    "这种每走一步都给予微小负奖励的设计，可以促使智能体寻找**最短路径**以最小化总惩罚。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trajectory（轨迹）与 Episode（回合）\n",
    "\n",
    "**Trajectory (轨迹)** 是智能体与环境交互产生的一系列状态、动作和奖励的序列：\n",
    "\n",
    "$$\n",
    "\\tau = s_1, a_1, r_1, s_2, a_2, r_2, s_3, a_3, r_3, \\dots\n",
    "$$\n",
    "\n",
    "**Episode (回合)**：如果任务有终止状态（如到达目标或游戏结束），那么从初始状态到终止状态的一个完整序列称为一个 Episode。\n",
    "\n",
    "#### Return（回报）\n",
    "强化学习的核心目标不是最大化单步奖励，而是最大化**累计回报 (Cumulative Return)**。\n",
    "\n",
    "$$ \n",
    "G_t = r_{t+1} + r_{t+2} + r_{t+3} + \\dots + r_T \n",
    "$$\n",
    "\n",
    "为了防止无限序列导致回报无穷大，以及体现“即时奖励比未来奖励更重要”的原则，我们引入 **Discount Factor (折扣因子) $\\gamma \\in [0, 1]$**：\n",
    "\n",
    "$$ \n",
    "G_t = r_{t+1} + \\gamma r_{t+2} + \\gamma^2 r_{t+3} + \\dots = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k+1} \n",
    "$$\n",
    "\n",
    "- $\\gamma = 0$：目光短浅，只关注下一步奖励。\n",
    "- $\\gamma \\to 1$：目光长远，关注未来所有奖励。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process (MDP)\n",
    "\n",
    "强化学习的问题通常被建模为 **马尔可夫决策过程 (MDP)**。MDP 是一个五元组 $(S, A, P, R, \\gamma)$：\n",
    "\n",
    "- $S$：状态集合 (State Space)\n",
    "- $A$：动作集合 (Action Space)\n",
    "- $P$：状态转移概率 (State Transition Probability)，即 $P(s_{t+1} | s_t, a_t)$\n",
    "- $R$：奖励函数 (Reward Function)，即 $R(s_t, a_t)$\n",
    "- $\\gamma$：折扣因子 (Discount Factor)\n",
    "\n",
    "#### 关键特性：马尔可夫性质 (Markov Property)\n",
    "MDP 假设环境具有**马尔可夫性质**，即：\n",
    "> \"The future is independent of the past given the present.\"\n",
    "> “在给定当前状态的情况下，未来的状态只与当前状态有关，而与过去的历史无关。”\n",
    "\n",
    "$$\n",
    "P(s_{t+1} | s_t, a_t, s_{t-1}, a_{t-1}, \\dots) = P(s_{t+1} | s_t, a_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从 MDP 退化为马尔可夫过程\n",
    "\n",
    "MDP 描述了带有**决策（Actions）**的过程。如果我们固定了策略 $\\pi$（即动作不再是选择变量，而是根据概率分布发生），MDP 就会退化。\n",
    "\n",
    "1. **Markov Process (MP, 马尔可夫过程)**：\n",
    "   - 只有状态和转移概率 $(S, P)$。\n",
    "   - 描述了状态随时间的随机演化序列。\n",
    "\n",
    "2. **Markov Reward Process (MRP, 马尔可夫奖励过程)**：\n",
    "   - 在 MP 的基础上加入了奖励：$(S, P, R, \\gamma)$。\n",
    "   - 当我们在 MDP 中给定一个策略 $\\pi$ 后，动作消失了（被整合进转移概率中），系统就变成了一个 MRP。这对后续评估一个策略的价值（Value Evaluation）非常重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent-Environment Interface (智能体与环境交互)\n",
    "\n",
    "强化学习的本质是智能体与环境的交互闭环：\n",
    "\n",
    "1. **Agent (智能体)**：\n",
    "   - 感知者：观察当前状态 $s_t$。\n",
    "   - 决策者：根据策略 $\\pi$ 选择动作 $a_t$。\n",
    "   - 学习者：根据奖励 $r_t$ 更新策略。\n",
    "\n",
    "2. **Environment (环境)**：\n",
    "   - 智能体之外的一切事物。\n",
    "   - 接收动作 $a_t$，反馈新的状态 $s_{t+1}$ 和奖励 $r_{t+1}$。\n",
    "\n",
    "这个循环不断进行：\n",
    "$$ s_0 \\xrightarrow{\\text{agent}} a_0 \\xrightarrow{\\text{env}} (r_1, s_1) \\xrightarrow{\\text{agent}} a_1 \\xrightarrow{\\text{env}} (r_2, s_2) \\dots $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本章小结与预告\n",
    "\n",
    "本章我们构建了强化学习的基础语言：\n",
    "- 我们在 Grid World 中定义了 **状态**、**动作** 和 **奖励**。\n",
    "- 我们了解了描述环境动态的 **状态转移** 和描述智能体行为的 **策略**。\n",
    "- 我们引出了 **MDP** 作为标准数学框架。\n",
    "\n",
    "在下一章中，我们将探讨如何通过 **贝尔曼方程 (Bellman Equation)** 来计算状态的价值，这是求解最优策略的关键。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}